# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: null
  - override /model: null
  - override /datamodule: null
  - override /callbacks: null
  - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "combined_human_vision_aug_dowsampled_cdf_aug_lr0.0001"
checkpoint_dir: "${work_dir}/storage/model/checkpoints/simbot/${name}"

seed: 12345

trainer:
  _target_: pytorch_lightning.Trainer

  gpus: [0]

  resume_from_checkpoint: null
  max_epochs: 80

  # number of validation steps to execute at the beginning of the training
  num_sanity_val_steps: 0
  gradient_clip_val: 1.0
  accumulate_grad_batches: 16
  strategy: ddp_find_unused_parameters_false
  precision: 16
  replace_sampler_ddp: False
  reload_dataloaders_every_n_epochs: 1
  limit_train_batches: 2048
  limit_test_batches: 7500
  check_val_every_n_epoch: 2

model:
  _target_: emma_policy.models.simbot_combined_policy.SimBotEmmaCombinedPolicy

  model_name: heriot-watt/emma-base
  # initialization_checkpoint: storage/model/checkpoints/emma_base_full_cc3mcocovg_balanced_bsz2048_lr0.0003_ratio3_no_global_embeds.ckpt
  initialization_checkpoint: storage/model/checkpoints/simbot/combined_human_vision_aug_dowsampled_cdf_aug_lr0.0001/epoch_075.ckpt
  num_beams: 5
  max_generated_text_length: 80
  lr: 0.0001
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler: linear_with_warmup
  num_warmup_steps: 0.1
  label_smoothing: 0.1
  save_results_path: combined_human_vision_aug_dowsampled_cdf_aug_lr0.0001_test_predictions.json
  strict: False
  resize_embeddings: True
  test_single_instance: True

datamodule:
  _target_: emma_policy.datamodules.simbot_combined_datamodule.SimBotCombinedDataModule

  model_name: heriot-watt/emma-base
  simbot_action_train_db_file: storage/db_emnlp/simbot_actions_train_annotations_human_vision_aug_downsampled_cdf_aug.db
  simbot_action_valid_db_file: storage/db_emnlp/simbot_actions_valid_annotations_human_vision_aug_downsampled_cdf_aug.db
  simbot_vad_train_db_file: storage/db_emnlp/simbot_clarifications_train_annotations_human_vision_aug_downsampled_cdf_aug.db
  simbot_vad_valid_db_file: storage/db_emnlp/simbot_clarifications_valid_annotations_human_vision_aug_downsampled_cdf_aug.db
  train_batch_size: 16
  val_batch_size: 1
  num_workers: 0
  max_lang_tokens: 128
  max_frames: 15
  tokenizer_truncation_side: right
  weighted_sampling: True
  weight_temperature: 1.3
  iou_threshold: 0.5
  skip_common_instances: False
  shuffle_objects: True

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "valid_exact_match" # name of the logged metric which determines when model is improving
    mode: "max" # "min" means higher metric value is better, can be also "max"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    verbose: False
    dirpath: "${checkpoint_dir}"
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False
  # early_stopping:
  #   _target_: pytorch_lightning.callbacks.EarlyStopping
  #   monitor: "train_loss" # name of the logged metric which determines when model is improving
  #   mode: "min" # "min" means higher metric value is better, can be also "max"
  #   patience: 100 # how many validation epochs of not improving until training stops
  #   min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
    refresh_rate_per_second: 5

  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: -1

  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "step"
  watch_model:
    _target_: emma_policy.callbacks.wandb.WatchModel
    log: null
    log_freq: 100
    log_graph: False

  upload_code_as_artifact:
    _target_: emma_policy.callbacks.wandb.artifacts.UploadCode
    code_dir: ${work_dir}/src

  upload_ckpts_as_artifact:
    _target_: emma_policy.callbacks.wandb.artifacts.UploadCheckpoints
    checkpoint_dir: "${checkpoint_dir}"
    only_upload_best: True

logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: "paper_simbot_combined"
    name: ${name}
    save_dir: "logs/"
    offline: True # set True to store all logs only locally
    id: null # pass correct id to resume experiment!
    entity: "emma-simbot"
    log_model: False
    job_type: "train"
