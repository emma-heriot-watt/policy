{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create the local and global masks for the enitre sequence assuming that input tokens are concatenated in the following order: [cnn scene tokens, object tokens, language tokens].\n",
    "\n",
    "Since we can only create the mask after the padding, we use scene, object and text temporal vectors with values:\n",
    "\n",
    "- -1 for history tokens\n",
    "- 0 for padding tokens\n",
    "- 1 .. N the number of the corresponding future frame \n",
    "\n",
    "Text input tokens will always be history tokens. We make text input tokens global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example with 3 history frames, 2 future frames and 1 padding token.\n",
    "scene_temporal_ids1 = torch.Tensor([-1, -1, -1, 1, 2, 0])\n",
    "# The history frames have a total of 6 objects, future frame 1 has 3 objects and future frame 2 has 2 objects.\n",
    "object_temporal_ids1 = torch.Tensor([-1, -1, -1, -1, -1, -1, 1, 1, 1, 2, 2, 0, 0])\n",
    "# There is no text for future frames.\n",
    "text_temporal_ids1 = torch.Tensor([-1, -1, -1, -1, -1, -1, -1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second sample with 2 history and 3 future frames\n",
    "scene_temporal_ids2 = torch.Tensor([-1, -1, 1, 2, 3, 0])\n",
    "object_temporal_ids2 = torch.Tensor([-1, -1, -1, -1, 1, 1, 1, 2, 2, 3, 0, 0, 0])\n",
    "text_temporal_ids2 = torch.Tensor([-1, -1, -1, -1, -1, 0, 0, 0, 0])\n",
    "\n",
    "# Concatenate them in a batch\n",
    "scene_temporal_ids = torch.stack([scene_temporal_ids1, scene_temporal_ids2])\n",
    "object_temporal_ids = torch.stack([object_temporal_ids1, object_temporal_ids2])\n",
    "text_temporal_ids = torch.stack([text_temporal_ids1, text_temporal_ids2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emma_policy.datamodules.collate import (\n",
    "    make_text_history_global_pattern,\n",
    "    make_encoder_causal_mask_batch,\n",
    ")\n",
    "\n",
    "attention2d = make_encoder_causal_mask_batch(\n",
    "    scene_temporal_ids,\n",
    "    object_temporal_ids,\n",
    "    text_temporal_ids,\n",
    "    dtype=scene_temporal_ids.dtype,\n",
    ")\n",
    "global_attenion = make_text_history_global_pattern(\n",
    "    scene_temporal_ids,\n",
    "    object_temporal_ids,\n",
    "    text_temporal_ids,\n",
    "    dtype=scene_temporal_ids.dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D ttention mask shape: batch size x total tokens x total tokens = torch.Size([2, 28, 28])\n",
      "Global ttention mask shapebatch size x total tokens =  torch.Size([2, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"2D ttention mask shape: batch size x total tokens x total tokens = {attention2d.shape}\")\n",
    "assert (\n",
    "    attention2d.shape[2]\n",
    "    == scene_temporal_ids.shape[1] + object_temporal_ids.shape[1] + text_temporal_ids.shape[1]\n",
    ")\n",
    "print(f\"Global ttention mask shapebatch size x total tokens =  {global_attenion.shape}\")\n",
    "assert (\n",
    "    global_attenion.shape[1]\n",
    "    == scene_temporal_ids.shape[1] + object_temporal_ids.shape[1] + text_temporal_ids.shape[1]\n",
    ")\n",
    "global_attenion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check any possible combination. The element (i, j) of the 2D attention mask is 1 if element i is allowed to attend to element j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in the batch.\n",
      "Scene-to-scene attention: 3 history frames, 2 future frames, 1 padding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First item in the batch.\")\n",
    "print(\"Scene-to-scene attention: 3 history frames, 2 future frames, 1 padding\")\n",
    "scene_len = scene_temporal_ids.shape[-1]\n",
    "attention2d[0, :scene_len, :scene_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene-to-objects attention: 6 history objects, 3 objects in frame 1, 2 objects in frame 2, 2 paddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    \"Scene-to-objects attention: 6 history objects, 3 objects in frame 1, 2 objects in frame 2, 2 paddings\"\n",
    ")\n",
    "object_len = object_temporal_ids.shape[-1]\n",
    "attention2d[0, :scene_len, scene_len : scene_len + object_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene-to-text attention: 7 history text tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Scene-to-text attention: 7 history text tokens\")\n",
    "text_len = text_temporal_ids.shape[-1]\n",
    "attention2d[0, :scene_len, scene_len + object_len :]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ee53ab302d70dc2b4b6ceff365a75f0f8d5471af86eaa2f96d460774c6ebc79"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('emma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
